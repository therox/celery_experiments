# Из чего состоит:
 ## Сервис поиска данных:
 * Опрос sentinel hub на предмет сущестования материалов по заданному контуру по заданной дате (списка дат)
 * Информация по каждому найденному датасету помещается в БД, при этом, поскольку поиска материалов на данную дату уже происходил, эта дата не будет участвовать в дальнейших поисках. Отсюда вывод - нам нужно каким-то образом учитывать каждую дату из периода запроса информации по полю. То есть необходимо учитывать даже те даты, на которые нет информации вообще
 * Создать задачу на скачивание датасета и послать ее в очередь celery

Возможно, будет необходимо потроить список дат, на которые МОЖЕТ БЫТЬ информация и обработать каждую из них с занесением соответствующей информации по каждой из них в БД.

## Сервис загрузки датасета
* Вытаскиваем задачу с датасетом.
* Пытаемся скачать датасет. Если получилось, оповещаем БД о том, где хранится датасет на S3 и создаем задачу на обработку (резку RGB, обсчет и резку NDVI, GNR, обсчет LAI, FAPAR). 

## Сервис обработки датасета
* Универсальный сервис, который умеет резать в своем составе все, но тем не менее, разбивается на отдельные таски, обслуживающие отдельные задачи из очереди.
После обработки данных (резка, расчет, обсчет, ...) создается задача на регистрацию данных (слоя) в GeoServer.

При этом, возможно параллельное использование локальной файловой системы для ускоренной обработки, то есть мы через прокидывание тома говорим, где в папке на локальной машине хранится этот датасет. Если этого датасета нет, остальные задачи, которым нужен датасет, скачивают его из S3. Тут нужно понять, стоит ли запускать задачи на обсчет параллельно, так как если локально файл не будет найден, то все параллелльно запущенные задачи будут пытаться скачивать его с s3, а нам нужно еще вовремя удалять локальные данные, не только если все задачи для этого датасета выполнились, но и все задачи для этого датасета запускались, но не все выполнились успешно по той или иной причине (невозможно посчитать индекс вледствие отсутствия определенных файлов, например). Наверное, имеет смысл запускать задачи последовательно в определенном порядке.

## Сервис регистрации слоев на GeoServer (до той поры, пока не будет решено отказаться от данного сервиса) 
* Сервис берет задачу на регистрацию слоя из очереди, пытается зарегистрировать слой, если не получается, то в зависимости от ошибки либо падает (и оставляет задачу в очереди), либо успешно завершается, тем самым давая понять, что в регистрации слоя на GeoServer нет необходимости.

## Список обработчиков задач
* Начальный задачер. Запускаем один раз, лезем в базу, если датасет не скачен, создаем задачу на скачку и пихаем в очередь `downloader`
* Скачиватель. Получаем задачу, скачиваем файло, кладем на S3, создаем задачу на обсчет NDVI, GNR, LAI, FAPAR (в зависимости от настроек) и пихаем в очередь `processing`
* Обработчик. Многофункциональный сервис, берет задачи из processing, обрабатывает их (считает индексы, создает файлы, кладет файлы на S3), по-окончании создает задачу на вырезку полей из соответствующих файлов (если это необходимо) и пихает в очередь `cutter`
* Резчик. Берет файл, контур поля, режет, кладет на геосерверный S3 и создает задачу на публикацию резанных слоев в геосервер
* Публикатор. Берет задачу, публикует слои, завершается
